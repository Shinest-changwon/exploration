{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "smart-girlfriend",
   "metadata": {},
   "source": [
    "# PROJECT 4. 네이버 영화리뷰 감성 분석 도전하기\n",
    "## 1) 데이터 수집 및 분석\n",
    "### (1) 데이터 수집\n",
    "데이터 셋은 네이버 영화의 댓글을 모아 구성된 **[Naver sentiment movie corpus](https://github.com/e9t/nsmc)** 입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "random-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터 읽기\n",
    "train_data = pd.read_table('~/aiffel/exploration/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/exploration/sentiment_classification/ratings_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-liverpool",
   "metadata": {},
   "source": [
    "### (2) 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "compliant-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 데이터 개수 : 150000\n",
      "test 데이터 개수 : 50000\n",
      "데이터 label 개수 : 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5403919</td>\n",
       "      <td>막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7797314</td>\n",
       "      <td>원작의 긴장감을 제대로 살려내지못했다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9443947</td>\n",
       "      <td>별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7156791</td>\n",
       "      <td>액션이 없는데도 재미 있는 몇안되는 영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5912145</td>\n",
       "      <td>왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
       "5   5403919      막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.      0\n",
       "6   7797314                              원작의 긴장감을 제대로 살려내지못했다.      0\n",
       "7   9443947  별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단...      0\n",
       "8   7156791                             액션이 없는데도 재미 있는 몇안되는 영화      1\n",
       "9   5912145      왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?      1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train 데이터 개수 :\", len(train_data))\n",
    "print(\"test 데이터 개수 :\", len(test_data))\n",
    "print(\"데이터 label 개수 :\", len((set(train_data[\"label\"]))))\n",
    "\n",
    "train_data.head(10)   # 데이터셋 10개 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-exclusion",
   "metadata": {},
   "source": [
    "데이터 셋은 영화에 대한 리뷰와 그에 대한 평가(0:부정, 1:긍정)입니다.  \n",
    "훈련 데이터는 150,000개 이고, 평가 데이터는 50,000개입니다.  \n",
    "그리고 데이터 라벨은 총 2개입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-enclosure",
   "metadata": {},
   "source": [
    "## 2) 데이터 전처리\n",
    "수집한 데이터들 중 일부는 중복값을 가지고 있거나 혹은 의미 없는 값을 가지고 있을 수도 있습니다.  \n",
    "이런 불필요한 데이터들을 처리하기 위해 데이터 전처리를 합니다.  \n",
    "자연어에서의 데이터 전처리는 토큰화, 정제, 정규화, 불용어 제거 등이 있습니다.  \n",
    "### (1) 데이터 정제 (cleaning)\n",
    "> 데이터 정제 : 데이터의 빈값(결측치)이나 정삼 범위를 벗어난 값(이상치)들을 제거하거나 다른 값으로 대체합니다.\n",
    "<br/>\n",
    "\n",
    "train 데이터와 test 데이터에 중복이 있는지 확인하고, 중복 데이터를 제거합니다.   \n",
    "그리고 null값을 가진 데이터가 있는지 확인하고, 결측치(Nan) 데이터를 제거합니다.  \n",
    "마지막으로 정규 표현식을 사용하여 구두점이나 특수문자, 공백을 제거합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "prompt-object",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================중복 제거====================\n",
      "중복 제거 전 train 데이터 개수 : 150000\n",
      "중복 제거 후 train 데이터 개수 : 146183\n",
      "\n",
      "중복 제거 전 test 데이터 개수 : 50000\n",
      "중복 제거 후 test 데이터 개수 : 49158\n",
      "===============================================\n",
      "\n",
      "==================결측치 제거====================\n",
      "결측치 제거 전 train 데이터 개수 : 146183\n",
      "결측치 제거 후 train 데이터 개수 : 146182\n",
      "\n",
      "결측치 제거 전 test 데이터 개수 : 49158\n",
      "결측치 제거 후 test 데이터 개수 : 49157\n",
      "===============================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146177</th>\n",
       "      <td>6222902</td>\n",
       "      <td>인간이 문제지 소는 뭔죄인가</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146178</th>\n",
       "      <td>8549745</td>\n",
       "      <td>평점이 너무 낮아서</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146179</th>\n",
       "      <td>9311800</td>\n",
       "      <td>이게 뭐요 한국인은 거들먹거리고 필리핀 혼혈은 착하다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146180</th>\n",
       "      <td>2376369</td>\n",
       "      <td>청춘 영화의 최고봉방황과 우울했던 날들의 자화상</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146181</th>\n",
       "      <td>9619869</td>\n",
       "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146182 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        9976970                                  아 더빙 진짜 짜증나네요 목소리      0\n",
       "1        3819312                         흠포스터보고 초딩영화줄오버연기조차 가볍지 않구나      1\n",
       "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3        9045019                          교도소 이야기구먼 솔직히 재미는 없다평점 조정      0\n",
       "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화스파이더맨에서 늙어보이기만 했던 커스틴 던...      1\n",
       "...          ...                                                ...    ...\n",
       "146177   6222902                                    인간이 문제지 소는 뭔죄인가      0\n",
       "146178   8549745                                         평점이 너무 낮아서      1\n",
       "146179   9311800                      이게 뭐요 한국인은 거들먹거리고 필리핀 혼혈은 착하다      0\n",
       "146180   2376369                         청춘 영화의 최고봉방황과 우울했던 날들의 자화상      1\n",
       "146181   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
       "\n",
       "[146182 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중복 제거하기\n",
    "print(\"====================중복 제거====================\")\n",
    "print(\"중복 제거 전 train 데이터 개수 :\", len(train_data))\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "print(\"중복 제거 후 train 데이터 개수 :\", len(train_data))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"중복 제거 전 test 데이터 개수 :\", len(test_data))\n",
    "test_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "print(\"중복 제거 후 test 데이터 개수 :\", len(test_data))\n",
    "print(\"===============================================\\n\")\n",
    "\n",
    "# null 제거하기\n",
    "print(\"==================결측치 제거====================\")\n",
    "print(\"결측치 제거 전 train 데이터 개수 :\", len(train_data))\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(\"결측치 제거 후 train 데이터 개수 :\", len(train_data))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"결측치 제거 전 test 데이터 개수 :\", len(test_data))\n",
    "test_data = test_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(\"결측치 제거 후 test 데이터 개수 :\", len(test_data))\n",
    "print(\"===============================================\\n\")\n",
    "\n",
    "# 정규 표현식\n",
    "train_data[\"document\"] = train_data[\"document\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")    # 한글과 공백을 제외하고 모두 제거\n",
    "test_data[\"document\"] = test_data[\"document\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")    # 한글과 공백을 제외하고 모두 제거\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)    # 인덱스 재정렬\n",
    "test_data = test_data.reset_index(drop=True)    # 인덱스 재정렬\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-junction",
   "metadata": {},
   "source": [
    "train 중복 데이터 개수 : 3817개  \n",
    "test 중복 데이터 개수 : 842개  \n",
    "train 결측 데이터 개수 : 1개  \n",
    "test 결측 데이터 개수 : 1개  \n",
    "  \n",
    "**총 train 데이터 개수 : 146182개**  \n",
    "**총 test 데이터 개수 : 49157개**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-collect",
   "metadata": {},
   "source": [
    "### (2) 데이터 토큰화 (tokenization) 및 불용어(Stopwords) 제거\n",
    "> 토큰화 : 텍스트를 문장이나 단어로 각각 분리합니다.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "예를 들어, \"I like an apple.\" 를 토큰화하면 \"I\", \"like\", \"an\", \"apple\", \".\" 이 됩니다.\n",
    "<br/>\n",
    "\n",
    "> 불용어 제거 : 불용어란 데이터 셋에 자주 등장하지만 분석에 큰 의미는 갖지 않는 단어를 말합니다.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;불용어가 다수 포함되어 있을수록 효율 감소, 처리시간 증가 등 악영향이 발생하므로 불용어를 제거합니다.\n",
    "\n",
    "Mecab 형태소 분석기를 사용하여 데이터 토큰화를 합니다.  \n",
    "그리고 데이터 토큰화를 하면서 불용어를 제거하도록 하겠습니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "quantitative-williams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화한 리뷰 5개 :\n",
      "['아', '더', '빙', '진짜', '짜증', '나', '네요', '목소리']\n",
      "['흠', '포스터', '보고', '초딩', '영화', '줄', '오버', '연기', '조차', '가볍', '지', '않', '구나']\n",
      "['너무', '재', '밓었다그래서보는것을추천한다']\n",
      "['교도소', '이야기', '구먼', '솔직히', '재미', '없', '다', '평점', '조정']\n",
      "['사이몬페그', '익살', '스런', '연기', '돋보였', '던', '영화', '스파이더맨', '에서', '늙', '어', '보이', '기', '만', '했', '던', '커스틴', '던스트', '너무나', '이뻐', '보였', '다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "tokenizer = Mecab()    # Mecab 형태소 분석기 불러오기 \n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']    # 불용어\n",
    "\n",
    "# 데이터 토큰화하기\n",
    "x_train = []    # 토큰화 된 훈련 데이터를 저장할 리스트\n",
    "for review in train_data[\"document\"]:\n",
    "    temp_x_train = tokenizer.morphs(review)    # 형태소 분석\n",
    "    temp_x_train = [word for word in temp_x_train if not word in stopwords]    # 형태소 분석을 통해 나온 단어들 중 불용어를 제외한 단어들 저장\n",
    "    x_train.append(temp_x_train)\n",
    "    \n",
    "# 데이터 토큰화하기\n",
    "x_test = []    # 토큰화 된 훈련 데이터를 저장할 리스트\n",
    "for review in test_data[\"document\"]:\n",
    "    temp_x_test = tokenizer.morphs(review)    # 형태소 분석\n",
    "    temp_x_test = [word for word in temp_x_test if not word in stopwords]    # 형태소 분석을 통해 나온 단어들 중 불용어를 제외한 단어들 저장\n",
    "    x_test.append(temp_x_test)\n",
    "    \n",
    "print(\"토큰화한 리뷰 5개 :\")\n",
    "for i in range(5):\n",
    "    print(x_train[i][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-upset",
   "metadata": {},
   "source": [
    "### (3) 정수 인코딩\n",
    "딥러닝 모델의 입력은 0과 1의 비트로 표현 되어있는 숫자만 가능합니다.  \n",
    "그래서 기계가 텍스트를 숫자로 처리할 수 있도록 데이터에 정수 인코딩을 수행해야 합니다.  \n",
    "토큰화된 데이터를 정수로 인코딩하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "rough-mason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전에 등록된 단어 개수 : 10000 개\n",
      "\n",
      "=====단어 빈도수 TOP50=====\n",
      "1등 : 영화\n",
      "2등 : 다\n",
      "3등 : 고\n",
      "4등 : 하\n",
      "5등 : 을\n",
      "6등 : 보\n",
      "7등 : 게\n",
      "8등 : 지\n",
      "9등 : 있\n",
      "10등 : 없\n",
      "11등 : 좋\n",
      "12등 : 나\n",
      "13등 : 었\n",
      "14등 : 만\n",
      "15등 : 는데\n",
      "16등 : 너무\n",
      "17등 : 봤\n",
      "18등 : 적\n",
      "19등 : 안\n",
      "20등 : 정말\n",
      "21등 : 로\n",
      "22등 : 것\n",
      "23등 : 음\n",
      "24등 : 아\n",
      "25등 : 네요\n",
      "26등 : 어\n",
      "27등 : 재밌\n",
      "28등 : 지만\n",
      "29등 : 같\n",
      "30등 : 진짜\n",
      "31등 : 에서\n",
      "32등 : 했\n",
      "33등 : 기\n",
      "34등 : 네\n",
      "35등 : 않\n",
      "36등 : 점\n",
      "37등 : 거\n",
      "38등 : 았\n",
      "39등 : 수\n",
      "40등 : 되\n",
      "41등 : 면\n",
      "42등 : ㅋㅋ\n",
      "43등 : 인\n",
      "44등 : 말\n",
      "45등 : 연기\n",
      "46등 : 주\n",
      "47등 : 최고\n",
      "48등 : 내\n",
      "49등 : 평점\n",
      "50등 : 이런\n",
      "========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 리뷰에 있는 단어들 중 사전에 없는 단어는 <UNK>로 바꾸기\n",
    "def wordlist_to_indexlist(wordlist):\n",
    "    return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "\n",
    "# 단어 빈도수 세기\n",
    "words = np.concatenate(x_train).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common(10000-4)\n",
    "vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}    # 사전(단어:인덱스)\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}    # 사전(인덱스:단어)\n",
    "\n",
    "x_train = list(map(wordlist_to_indexlist, x_train))\n",
    "x_test = list(map(wordlist_to_indexlist, x_test))\n",
    "\n",
    "top5 = [index_to_word[i+4] for i in range(50)]    # 단어 빈도수 top 100\n",
    "\n",
    "print(\"사전에 등록된 단어 개수 :\", len(word_to_index), \"개\")\n",
    "\n",
    "print(\"\\n=====단어 빈도수 TOP50=====\")\n",
    "for i in range(len(top5)):\n",
    "    print(\"{}등 : {}\".format(i+1, top5[i]))\n",
    "print(\"========================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-johnson",
   "metadata": {},
   "source": [
    "### (4) 패딩 (Padding)\n",
    "> 패딩 : 가변적 길이를 가지는 문장을 같은 길이로 맞춰주는 작업\n",
    "<br/>\n",
    "\n",
    "자연어 처리를 하다보면 데이터의 각 문장은 서로 길이가 다를 수 있습니다.  \n",
    "기계는 데이터를 행렬로 보기 때문에, 문장들의 길이를 전부 동일하게 해주는 패딩 작업을 해야합니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "intended-demonstration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 83\n",
      "리뷰의 평균 길이 : 13.725663898427987\n",
      "pad_sequences maxlen :  36\n"
     ]
    }
   ],
   "source": [
    "# 패딩 길이 정하기\n",
    "\n",
    "len_result = [len(s) for s in x_train]\n",
    "\n",
    "print(\"리뷰의 최대 길이 :\", np.max(len_result))\n",
    "print(\"리뷰의 평균 길이 :\", np.mean(len_result))\n",
    "\n",
    "# 최대 길이 = (평균 + 2*표준편차) \n",
    "max_tokens = np.mean(len_result) + 2 * np.std(len_result)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "human-hotel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146182, 36)\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  27  67 895  33 214  15  28 699]\n"
     ]
    }
   ],
   "source": [
    "# 패딩 추가하기\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "y_train = np.array(list(train_data['label']))\n",
    "y_test = np.array(list(test_data['label']))\n",
    "print(x_train.shape)\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-diamond",
   "metadata": {},
   "source": [
    "리뷰의 길이를 모두 36으로 맞춰준 것을 확인할 수 있습니다.  \n",
    "또한 길이가 36이 안되는 리뷰는 앞쪽에 <PAD> 값을 넣어 길이를 36으로 맞춘 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-transcription",
   "metadata": {},
   "source": [
    "## 4) 모델링 및 학습\n",
    "전처리된 데이터를 이용하여 딥러닝 모델을 학습시킵니다.  \n",
    "딥러닝 모델은 3가지(RNN, CNN, LST)를 사용할 것입니다.  \n",
    "\n",
    "### (1) 모델링 (Modeling)\n",
    "> CNN : Convolution Neural Network의 약자로 합성곱 연산을 사용하는 인공신경망 중 하나입니다.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "합성곱이란 하나의 함수와 또 다른 함수를 반전 이동한 값을 곱한 다음, 구간에 대해 적분하여 새로운 함수를 구하는 연산자입니다.\n",
    "<br/>\n",
    "\n",
    "> RNN : Recurrent Neural Netowrk의 약자로 입력과 출력을 시퀀스 단위로 처리하는 모델입니다.\n",
    "<br/>\n",
    "\n",
    "> LSTM : Long Short-Term Memory의 약자로 RNN의 장기 의존성 문제를 보완한 모델입니다. \n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "impressed-medicaid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_109\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_109 (Embedding)    (None, None, 14)          140000    \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 128)               73216     \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 229,857\n",
      "Trainable params: 229,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Model: \"sequential_110\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_110 (Embedding)    (None, None, 14)          140000    \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, None, 128)         1920      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, None, 256)         33024     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_36 (Glo (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_189 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_190 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 207,969\n",
      "Trainable params: 207,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Model: \"sequential_111\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_111 (Embedding)    (None, None, 14)          140000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_36 (SimpleRNN)    (None, 125)               17500     \n",
      "_________________________________________________________________\n",
      "dense_191 (Dense)            (None, 128)               16128     \n",
      "_________________________________________________________________\n",
      "dense_192 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 173,757\n",
      "Trainable params: 173,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝 모델 설계하기\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 14  # 워드 벡터의 차원수\n",
    "\n",
    "# LSTM 모델\n",
    "lstm_model = keras.Sequential()\n",
    "lstm_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "lstm_model.add(keras.layers.LSTM(128))\n",
    "lstm_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "lstm_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model.summary()\n",
    "print()\n",
    "\n",
    "# CNN 모델\n",
    "cnn_model = keras.Sequential()\n",
    "cnn_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "cnn_model.add(keras.layers.Conv1D(128, 1, activation='relu'))\n",
    "cnn_model.add(keras.layers.MaxPooling1D())\n",
    "cnn_model.add(keras.layers.Conv1D(256, 1, activation='relu'))\n",
    "cnn_model.add(keras.layers.GlobalMaxPooling1D())\n",
    "cnn_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "cnn_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn_model.summary()\n",
    "print()\n",
    "\n",
    "# RNN 모델\n",
    "rnn_model = keras.Sequential()\n",
    "rnn_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "rnn_model.add(SimpleRNN(125))\n",
    "rnn_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "rnn_model.summary()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-mozambique",
   "metadata": {},
   "source": [
    "### (2) 학습 (train)\n",
    "위에서 설계한 모델들(RNN, CNN, LSTM)을 사용하여 학습시킵니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "stock-partner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136182, 36)\n",
      "(136182,)\n"
     ]
    }
   ],
   "source": [
    "# model 훈련전, 훈련용 데이터 셋 146182건 중 30000건을 분리하여 검증셋으로 사용합니다.\n",
    "\n",
    "# validation set 30000건 분리\n",
    "x_val = x_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 \n",
    "partial_x_train = x_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "impossible-adrian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "266/266 [==============================] - 4s 15ms/step - loss: 0.4663 - accuracy: 0.7639 - val_loss: 0.3574 - val_accuracy: 0.8414\n",
      "Epoch 2/3\n",
      "266/266 [==============================] - 4s 15ms/step - loss: 0.3374 - accuracy: 0.8539 - val_loss: 0.3541 - val_accuracy: 0.8428\n",
      "Epoch 3/3\n",
      "266/266 [==============================] - 4s 15ms/step - loss: 0.3078 - accuracy: 0.8700 - val_loss: 0.3678 - val_accuracy: 0.8407\n",
      "1537/1537 - 4s - loss: 0.3842 - accuracy: 0.8336\n",
      "Epoch 1/3\n",
      "266/266 [==============================] - 12s 43ms/step - loss: 0.4400 - accuracy: 0.7903 - val_loss: 0.3623 - val_accuracy: 0.8394\n",
      "Epoch 2/3\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.3409 - accuracy: 0.8517 - val_loss: 0.3621 - val_accuracy: 0.8380\n",
      "Epoch 3/3\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.3125 - accuracy: 0.8664 - val_loss: 0.3558 - val_accuracy: 0.8455\n",
      "1537/1537 - 4s - loss: 0.3666 - accuracy: 0.8379\n",
      "Epoch 1/3\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.4336 - accuracy: 0.7935 - val_loss: 0.3545 - val_accuracy: 0.8414\n",
      "Epoch 2/3\n",
      "266/266 [==============================] - 2s 7ms/step - loss: 0.3429 - accuracy: 0.8515 - val_loss: 0.3483 - val_accuracy: 0.8449\n",
      "Epoch 3/3\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.3275 - accuracy: 0.8590 - val_loss: 0.3488 - val_accuracy: 0.8435\n",
      "1537/1537 - 2s - loss: 0.3615 - accuracy: 0.8391\n",
      "\n",
      "RNN 정확도 : 0.8336350917816162\n",
      "CNN 정확도 : 0.8379477858543396\n",
      "LSTM 정확도 : 0.8391277194023132\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "epochs=3\n",
    "\n",
    "# RNN 모델 학습\n",
    "rnn_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "rnn_history = rnn_model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "\n",
    "rnn_results = rnn_model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "# CNN 모델 학습\n",
    "cnn_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn_history = cnn_model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "\n",
    "cnn_results = cnn_model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "# LSTM 모델 학습\n",
    "lstm_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "lstm_history = lstm_model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "\n",
    "lstm_results = lstm_model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print()\n",
    "print(\"RNN 정확도 :\", rnn_results[1])\n",
    "print(\"CNN 정확도 :\", cnn_results[1])\n",
    "print(\"LSTM 정확도 :\", lstm_results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-content",
   "metadata": {},
   "source": [
    "**정확도는 LSTM > CNN > RNN 순으로 높습니다. 하지만 거의 비슷한 정확도입니다.**  \n",
    "**3가지 모델 중 LSTM의 정확도가 가장 높지만, 우리가 원하고자 정확도(85%이상)에는 미치지 못합니다.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-chemical",
   "metadata": {},
   "source": [
    "## 5) 학습된 임베딩 레이어 분석\n",
    "gensim을 활용하여 자체학습된 임베딩 레이어를 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "little-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "# RNN 임베딩 레이어 분석\n",
    "embedding_layer = rnn_model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "rnn_word2vec_file_path = os.getenv('HOME')+'/aiffel/exploration/sentiment_classification/rnn_word2vec.txt'\n",
    "f = open(rnn_word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))\n",
    "\n",
    "vectors = rnn_model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "rnn_word_vectors = Word2VecKeyedVectors.load_word2vec_format(rnn_word2vec_file_path, binary=False)\n",
    "\n",
    "# CNN 임베딩 레이어 분석\n",
    "embedding_layer = cnn_model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "cnn_word2vec_file_path = os.getenv('HOME')+'/aiffel/exploration/sentiment_classification/cnn_word2vec.txt'\n",
    "f = open(cnn_word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))\n",
    "\n",
    "vectors = cnn_model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "cnn_word_vectors = Word2VecKeyedVectors.load_word2vec_format(cnn_word2vec_file_path, binary=False)\n",
    "\n",
    "# LSTM 임베딩 레이어 분석\n",
    "embedding_layer = lstm_model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "lstm_word2vec_file_path = os.getenv('HOME')+'/aiffel/exploration/sentiment_classification/lstm_word2vec.txt'\n",
    "f = open(lstm_word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))\n",
    "\n",
    "vectors = lstm_model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "lstm_word_vectors = Word2VecKeyedVectors.load_word2vec_format(lstm_word2vec_file_path, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "continuous-greenhouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN 차원 : 14\n",
      "CNN 차원 : 14\n",
      "LSTM 차원 : 14\n",
      "\n",
      "(RNN)'최고'라는 단어와 유사한 단어들 : ['담백', '수작', '지연', '최상', '굿', '각지', '클레이', '웰메이드', '일어서', '명품']\n",
      "(CNN)'최고'라는 단어와 유사한 단어들 : ['주몽', '한꺼번에', '각지', '고든', '레고', '그곳', '손색', '슬펐', '후딱', '사나']\n",
      "(LSTM)'최고'라는 단어와 유사한 단어들 : ['흥겨운', '비로소', '하나하나', '김현주', '담백', '맛깔', '기막힌', '부패', '고조', '십수']\n",
      "\n",
      "(RNN)'지루'라는 단어와 유사한 단어들 : ['철없', '유치', '썰렁', '으으으', '임요환', '대견', '잣', '빼앗', '싫', '애매']\n",
      "(CNN)'지루'라는 단어와 유사한 단어들 : ['려니', '려구', '글쎄요', '비호', '환불', '책임', '아까', '불면증', '수치심', '드러워']\n",
      "(LSTM)'지루'라는 단어와 유사한 단어들 : ['망쳤', '돌렸', '경력', '별루', '문란', '오유', '끝냈', '루지', '지루함', '멀미']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RNN 차원 :\", len(rnn_word_vectors['영화']))\n",
    "print(\"CNN 차원 :\", len(cnn_word_vectors['영화']))\n",
    "print(\"LSTM 차원 :\", len(lstm_word_vectors['영화']))\n",
    "print()\n",
    "\n",
    "temp = rnn_word_vectors.similar_by_word(\"최고\")\n",
    "print(\"(RNN)'최고'라는 단어와 유사한 단어들 :\", [temp[i][0] for i in range(len(temp))])\n",
    "temp = cnn_word_vectors.similar_by_word(\"최고\")\n",
    "print(\"(CNN)'최고'라는 단어와 유사한 단어들 :\", [temp[i][0] for i in range(len(temp))])\n",
    "temp = lstm_word_vectors.similar_by_word(\"최고\")\n",
    "print(\"(LSTM)'최고'라는 단어와 유사한 단어들 :\", [temp[i][0] for i in range(len(temp))])\n",
    "print()\n",
    "\n",
    "temp = rnn_word_vectors.similar_by_word(\"지루\")\n",
    "print(\"(RNN)'지루'라는 단어와 유사한 단어들 :\", [temp[i][0] for i in range(len(temp))])\n",
    "temp = cnn_word_vectors.similar_by_word(\"지루\")\n",
    "print(\"(CNN)'지루'라는 단어와 유사한 단어들 :\", [temp[i][0] for i in range(len(temp))])\n",
    "temp = lstm_word_vectors.similar_by_word(\"지루\")\n",
    "print(\"(LSTM)'지루'라는 단어와 유사한 단어들 :\", [temp[i][0] for i in range(len(temp))])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-surprise",
   "metadata": {},
   "source": [
    "**RNN - '최고' : '굿', '최상' 등 유사한 단어가 있습니다. 하지만 그 외의 단어는 유사한 단어가 아닌것 같습니다.**  \n",
    "**CNN - '최고' : 모든 단어가 전혀 관련이 없는 단어들인 것같습니다.**  \n",
    "**LSTM - '최고' : 모든 단어가 전혀 관련이 없는 단어들인 것같습니다.**  \n",
    "\n",
    "**RNN - '지루' : '썰렁', '유치' 등 유사한 단어가 있습니다. 하지만 그 외의 단어는 유사한 단어가 아닌것 같습니다.**  \n",
    "**CNN - '지루' : 전혀 관련이 없는 단어들인 것같습니다.**  \n",
    "**LSTM - '지루' : '지루함', '멀미' 등 유사한 단어가 있습니다. 하지만 그 외의 단어는 유사한 단어가 아닌것 같습니다.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-inventory",
   "metadata": {},
   "source": [
    "## 6) 한국어 Word2Vec 임베딩 활용하기\n",
    "워드 벡터를 더 정교하게 학습시키기 위해서 **[한국어 Word2Vec](https://github.com/Kyubyong/wordvectors)** 사이트에 있는 사전학습된 워드 임베딩 모델을 가져와서 활용해 보도록 하겠습니다.   \n",
    "\n",
    "### (1) 워드투벡터 (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "knowing-biology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec 차원 : 200\n",
      "\n",
      "'최고'라는 단어와 유사한 단어들 : ['최대', '최강', '유일한', '일류', '최악', '최연소', '랭킹', '제일의', '최초', '최상']\n",
      "'지루'라는 단어와 유사한 단어들 : ['답답', '편안', '솔직', '쓸쓸', '차분', '조용', '냉정', '자유분방', '피곤', '느긋']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/exploration/sentiment_classification/ko/ko.bin'\n",
    "word2vec = Word2Vec.load(word2vec_path)\n",
    "\n",
    "print(\"Word2Vec 차원 :\", len(word2vec.wv[\"영화\"]))\n",
    "print()\n",
    "\n",
    "temp = word2vec.wv.most_similar(\"최고\")\n",
    "print(\"'최고'라는 단어와 유사한 단어들 :\", [temp[i][0] for i in range(len(temp))])\n",
    "\n",
    "temp = word2vec.wv.most_similar(\"지루\")\n",
    "print(\"'지루'라는 단어와 유사한 단어들 :\", [temp[i][0] for i in range(len(temp))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "formal-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기 (10,000개의 단어)\n",
    "word_vector_dim = 200  # 워드 벡터의 차원수\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec.wv:\n",
    "        embedding_matrix[i] = word2vec.wv[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-chicken",
   "metadata": {},
   "source": [
    "### (2) 모델링 (Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "difficult-major",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_112\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_112 (Embedding)    (None, None, 200)         2000000   \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 128)               168448    \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,185,089\n",
      "Trainable params: 2,185,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Model: \"sequential_113\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_113 (Embedding)    (None, None, 200)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, None, 128)         25728     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, None, 256)         33024     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_37 (Glo (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,091,777\n",
      "Trainable params: 2,091,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Model: \"sequential_114\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_114 (Embedding)    (None, None, 200)         2000000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_37 (SimpleRNN)    (None, 125)               40750     \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 128)               16128     \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,057,007\n",
      "Trainable params: 2,057,007\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝 모델 설계하기\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200  # 워드 벡터의 차원수\n",
    "\n",
    "# LSTM 모델\n",
    "lstm_model = keras.Sequential()\n",
    "lstm_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "lstm_model.add(keras.layers.LSTM(128))\n",
    "lstm_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "lstm_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model.summary()\n",
    "print()\n",
    "\n",
    "# CNN 모델\n",
    "cnn_model = keras.Sequential()\n",
    "cnn_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "cnn_model.add(keras.layers.Conv1D(128, 1, activation='relu'))\n",
    "cnn_model.add(keras.layers.MaxPooling1D())\n",
    "cnn_model.add(keras.layers.Conv1D(256, 1, activation='relu'))\n",
    "cnn_model.add(keras.layers.GlobalMaxPooling1D())\n",
    "cnn_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "cnn_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn_model.summary()\n",
    "print()\n",
    "\n",
    "# RNN 모델\n",
    "rnn_model = keras.Sequential()\n",
    "rnn_model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "rnn_model.add(SimpleRNN(125))\n",
    "rnn_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "rnn_model.summary()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-government",
   "metadata": {},
   "source": [
    "### (2) 학습 (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "friendly-painting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "266/266 [==============================] - 10s 36ms/step - loss: 0.4299 - accuracy: 0.7934 - val_loss: 0.3486 - val_accuracy: 0.8435\n",
      "Epoch 2/3\n",
      "266/266 [==============================] - 9s 35ms/step - loss: 0.3103 - accuracy: 0.8663 - val_loss: 0.3735 - val_accuracy: 0.8357\n",
      "Epoch 3/3\n",
      "266/266 [==============================] - 9s 35ms/step - loss: 0.2233 - accuracy: 0.9089 - val_loss: 0.4223 - val_accuracy: 0.8330\n",
      "1537/1537 - 5s - loss: 0.4337 - accuracy: 0.8266\n",
      "Epoch 1/3\n",
      "266/266 [==============================] - 12s 44ms/step - loss: 0.4137 - accuracy: 0.8076 - val_loss: 0.3518 - val_accuracy: 0.8456\n",
      "Epoch 2/3\n",
      "266/266 [==============================] - 6s 23ms/step - loss: 0.3208 - accuracy: 0.8627 - val_loss: 0.3447 - val_accuracy: 0.8467\n",
      "Epoch 3/3\n",
      "266/266 [==============================] - 6s 23ms/step - loss: 0.2789 - accuracy: 0.8844 - val_loss: 0.3584 - val_accuracy: 0.8473\n",
      "1537/1537 - 3s - loss: 0.3686 - accuracy: 0.8415\n",
      "Epoch 1/3\n",
      "266/266 [==============================] - 7s 27ms/step - loss: 0.4074 - accuracy: 0.8093 - val_loss: 0.3455 - val_accuracy: 0.8446\n",
      "Epoch 2/3\n",
      "266/266 [==============================] - 7s 26ms/step - loss: 0.3227 - accuracy: 0.8585 - val_loss: 0.3369 - val_accuracy: 0.8514\n",
      "Epoch 3/3\n",
      "266/266 [==============================] - 7s 26ms/step - loss: 0.2851 - accuracy: 0.8761 - val_loss: 0.3395 - val_accuracy: 0.8475\n",
      "1537/1537 - 3s - loss: 0.3440 - accuracy: 0.8496\n",
      "\n",
      "RNN 정확도 : 0.8265760540962219\n",
      "CNN 정확도 : 0.841487467288971\n",
      "LSTM 정확도 : 0.8496450185775757\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "epochs=3\n",
    "\n",
    "# RNN 모델 학습\n",
    "rnn_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "rnn_history = rnn_model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "\n",
    "rnn_results = rnn_model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "# CNN 모델 학습\n",
    "cnn_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn_history = cnn_model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "\n",
    "cnn_results = cnn_model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "# LSTM 모델 학습\n",
    "lstm_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "lstm_history = lstm_model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)\n",
    "\n",
    "lstm_results = lstm_model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print()\n",
    "print(\"RNN 정확도 :\", rnn_results[1])\n",
    "print(\"CNN 정확도 :\", cnn_results[1])\n",
    "print(\"LSTM 정확도 :\", lstm_results[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-concentration",
   "metadata": {},
   "source": [
    "## [ 결과  - 루브릭] \n",
    "| Model | Word2Vec 사용전 | Word2Vec 사용후 | accuracy 변화율 |\n",
    "|:----------|:----------:|:----------:|:----------:|\n",
    "| RNN | 83.36% | 82.66% | -0.7% |\n",
    "| CNN | 83.79% | 84.15% | +0.36% |\n",
    "| LSTM | 83.91% | 84.96% | +1.05% |  \n",
    "<br/>\n",
    "\n",
    "#### 1. 다양한 방법으로 Text Classification 태스크를 성공적으로 구현하였다.\n",
    "- 3가지 이상의 모델(RNN, LSTM,, CNN)을 성공적으로 시도하였습니다. :)\n",
    "\n",
    "#### 2. gensim을 활용하여 자체학습된 혹은 사전학습된 임베딩 레이어를 분석하였다.\n",
    "- gensim의 유사단어 찾기를 활용하여 사전학습된 임베딩을 분석하였습니다. :)\n",
    "\n",
    "#### 3. 한국어 Word2Vec을 활용하여 가시적인 성능향상을 달성했다.\n",
    "- 네이버 영화리뷰 데이터 감성분석 정확도를 85% 이상 달성하지 못했습니다. :(  \n",
    "    *(ps. LSTM의 정확도가 84.96%인데... 반올림하면 85%인데... 85%라고 해도 될 것 같은데... ㅠ.ㅠ)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-metadata",
   "metadata": {},
   "source": [
    "## [느낀점]\n",
    "Word2Vec을 사용했음에도 불구하고, 정확도가 많이 향상되지 않았습니다.  \n",
    "그래서 정확도가 왜 많이 향상되지 않았을까 그 이유에 대해 생각을 해보았습니다.  \n",
    "#### 1. 불용어를 더 제거해야한다고 생각합니다.  \n",
    "    위의 소스코드에서 빈도수 단어 TOP5를 출력했을때, '다', '수', '인', '을' 등 긍부정을 분류하기에는 좋지 않은 단어들이 있었습니다.\n",
    "    이러한 단어들이 긍부정을 분류할때, 방해가 된것 같습니다.  \n",
    "    제 생각에는 조사나 접미사 같은 단어들을 더 제거해야 높은 정확도를 얻을 수 있다고 생각합니다.\n",
    "#### 2. 긍부정을 분류할 때, 문장안에 있는 단어의 유사도보다는 문장이 어떤 유형의 글인지가 더 중요하다고 생각합니다.\n",
    "    영화와 관련된 문장, 스포츠와 관련된 문장, 연예인과 관련된 문장 등 세상에는 여러 카테고리의 문장들이 있습니다.  \n",
    "    각 카테고리별로 문장의 긍부정을 분류할 때, 긍부정을 판별하는 feature는 서로 다를 수 있습니다.  \n",
    "    예를 들어, '대박', '쩐다'와 같은 단어는 영화리뷰에서 긍정단어로 사용되지만, 연예인관련기사에서는 부정단어로 사용될 수 있습니다.  \n",
    "    왜냐하면 \"진짜 대박이네...\" 같은 문장은 연예인에게 실망했을때 쓰일 수 있고, \"오바쩐다.\" 같은 문장은 연예인을 안좋게 볼때 쓰일 수 있기 때문입니다. \n",
    "    \n",
    "> **정확도 85%를 달성하지 못해서 아쉽지만, 그로 인해 깨닫게 된 것들이 많이 있어서 도움이 되었습니다. :)**\n",
    "\n",
    "정확도를 올리려고 validation set, epoch, model input 등 다 바꿔봤는데도 안오르네요....  \n",
    "정말 힘든 프로젝트였습니다... ㅠㅠ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
